from __future__ import print_function
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import pickle, random
import matplotlib.pyplot as plt

def tf_confusion_metrics(predict, real, session, feed_dict):
    predictions = tf.argmax(predict, 1)
    actuals = tf.argmax(real, 1)

    ones_like_actuals = tf.ones_like(actuals)
    zeros_like_actuals = tf.zeros_like(actuals)
    ones_like_predictions = tf.ones_like(predictions)
    zeros_like_predictions = tf.zeros_like(predictions)

    tp_op = tf.reduce_sum(
        tf.cast(
            tf.logical_and(
                tf.equal(actuals, ones_like_actuals),
                tf.equal(predictions, ones_like_predictions)
            ),
            "float"
        )
    )

    tn_op = tf.reduce_sum(
        tf.cast(
            tf.logical_and(
                tf.equal(actuals, zeros_like_actuals),
                tf.equal(predictions, zeros_like_predictions)
            ),
            "float"
        )
    )

    fp_op = tf.reduce_sum(
        tf.cast(
            tf.logical_and(
                tf.equal(actuals, zeros_like_actuals),
                tf.equal(predictions, ones_like_predictions)
            ),
            "float"
        )
    )

    fn_op = tf.reduce_sum(
        tf.cast(
            tf.logical_and(
                tf.equal(actuals, ones_like_actuals),
                tf.equal(predictions, zeros_like_predictions)
            ),
            "float"
        )
    )
    tp, tn, fp, fn = session.run([tp_op, tn_op, fp_op, fn_op], feed_dict)

    tpr = float(tp)/(float(tp) + float(fn))
    fpr = float(fp)/(float(fp) + float(tn))
    fnr = float(fn)/(float(tp) + float(fn))

    accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))

    recall = tpr
    precision = float(tp)/(float(tp) + float(fp))

    f1_score = (2 * (precision * recall)) / (precision + recall)

    print("tp =", tp, "tn =", tn, "fp =", fp, "fn =", fn)
    print("accuracy =", accuracy, "precision =", precision, "recall =", recall, "f1_score =", f1_score)


Bengin_sentences = pickle.load(open("BMsave/Bengin_Android_OPCodeList", "r+b"))
Malware_sentences = pickle.load(open("BMsave/Malware_Android_OPCodeList", "r+b"))
Android_vocabulary = pickle.load(open("BMsave/Android_vocabulary", "r+b"))

Y = []
for i in range(len(Bengin_sentences)):
    Y.append([1, 0])
for i in range(len(Malware_sentences)):
    Y.append([0, 1])

vectorizer = CountVectorizer(min_df=0, lowercase=False, vocabulary=Android_vocabulary)
# for i in PC_vectorizer.vocabulary_:
#     print(i)

X_train, X_test, Y_train, Y_test = train_test_split(np.array(vectorizer.transform(Bengin_sentences + Malware_sentences).toarray(), dtype=np.float64), Y, test_size=0.75, random_state=100)

min_max_scaler = MinMaxScaler()
X_train = min_max_scaler.fit_transform(X_train)
X_test = min_max_scaler.fit_transform(X_test)
# Y_train = np.transpose(Y_train)
# Y_test = np.transpose(Y_test)
PC_Weights = pickle.load(open("BMsave\PC_Weights", "r+b"))

Init_Weights = []
for i in range(61):
    if i in range(36):
        Init_Weights.append(PC_Weights[i])
    else:
        temp = []
        for j in range(30):
            temp.append(random.normalvariate(0, 0.1))
        Init_Weights.append(temp)

Init_Weights = np.asarray(Init_Weights, dtype=np.float32)
tensorval = tf.constant(value=Init_Weights, dtype=tf.float32)

# define placeholder for inputs to network
inputLayer = tf.placeholder(tf.float32, [None, 61])
# hiddenWeight = tf.Variable(tf.truncated_normal([61, 30], mean=0, stddev=0.1), name="hiddenWeight")
hiddenWeight = tf.Variable(tensorval, name="hiddenWeight")
hiddenBias = tf.Variable(tf.truncated_normal([30]), name="hiddenBias")
hiddenLayer = tf.add(tf.matmul(inputLayer, hiddenWeight), hiddenBias)
hiddenLayer = tf.nn.sigmoid(hiddenLayer)
outputWeight = tf.Variable(tf.truncated_normal([30, 2], mean=0, stddev=0.1))
outputBias = tf.Variable(tf.truncated_normal([2], mean=0, stddev=0.1))
outputLayer = tf.add(tf.matmul(hiddenLayer, outputWeight), outputBias)
outputLayer = tf.nn.sigmoid(outputLayer)
outputLabel = tf.placeholder(tf.float32, shape=[None, 2])


# the error between prediction and real data
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=outputLabel, logits=outputLayer))
optimizer = tf.train.AdamOptimizer()
target = optimizer.minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

acc1 = []
LossList1 = []
acc2 = []
LossList2 = []
for i in range(5000):
    x = X_train
    y = Y_train
    sess.run(target, feed_dict={inputLayer: x, outputLabel: y})
    if (i+1) % 20 == 0:
        corrected = tf.equal(tf.argmax(outputLabel, 1), tf.argmax(outputLayer, 1))
        accuracy = tf.reduce_mean(tf.cast(corrected, tf.float32))
        accuracyValue, lossValue = sess.run([accuracy, loss], feed_dict={inputLayer: X_train, outputLabel: Y_train})
        print(i+1, 'train set accuracy:', accuracyValue)
        acc1.append(accuracyValue)
        LossList1.append(lossValue)
        accuracyValue, lossValue = sess.run([accuracy, loss], feed_dict={inputLayer: X_test, outputLabel: Y_test})
        acc2.append(accuracyValue)
        LossList2.append(lossValue)

tf_confusion_metrics(outputLayer, Y_test, sess, feed_dict={inputLayer: X_test, outputLabel: Y_test})
W = tf.get_default_graph().get_tensor_by_name('hiddenWeight:0').eval(session=sess)
pickle.dump(W, open('BMsave\PC_Weights', 'w+b'))
# fig = plt.figure()
# plt.plot(range(250), acc1)
# plt.plot(range(250), acc2)
# plt.xlabel("x")
# plt.ylabel("acc")
# plt.show()
# fig = plt.figure()
# plt.plot(range(250), LossList1)
# plt.plot(range(250), LossList2)
# plt.xlabel("x")
# plt.ylabel("loss")
# plt.show()
# for i in acc1:
#     print(i)
# print()
# print()
# for i in acc2:
#     print(i)
# print()
# print()
# print("*******************************************")
# for i in LossList1:
#     print(i)
# print()
# print()
# for i in LossList2:
#     print(i)
# print()
# print()


